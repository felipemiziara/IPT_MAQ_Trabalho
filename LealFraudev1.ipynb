{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf5849a5-89c7-4b1b-831b-d1568ac063db",
   "metadata": {},
   "source": [
    "# Treinamento de Fraudes com Transformers\n",
    "\n",
    "*Autor: Prof. Dr. Adriano Leal\n",
    "\n",
    "Data de Criação: 13 de Abril de 2024\n",
    "\n",
    "---\n",
    "\n",
    "## Descrição\n",
    "Este notebook Jupyter foi desenvolvido para automatizar .\n",
    "\n",
    "## Not Working yet\n",
    "---\n",
    "\n",
    "## Log de Modificações\n",
    "\n",
    "- **13/04/2024 - Criação inicial do notebook e implementação das funções básicas de importação por Adriano Leal.\n",
    "- **[Data da Modificação]** - Adicionado suporte para logs de erro detalhados e tratamento de exceções por [Nome do Colaborador ou Seu Nome].\n",
    "- **[Data da Modificação]** - Melhoria na interface de seleção de arquivos e otimização de performance por [Nome do Colaborador ou Seu Nome].\n",
    "\n",
    "---\n",
    "\n",
    "## Notas Adicionais\n",
    "Este notebook é parte do projeto [Nome do Projeto] destinado a melhorar a gestão e análise de dados relacionados a deslizamentos.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df228a4-ec19-4a10-873a-ab6de9c953f3",
   "metadata": {},
   "source": [
    "!pip install -U  \"ray[tune]\" tensorboardX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c69960e-9683-4fe5-b5a5-91cbaa338782",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/nvidia/cudnn/__init__.py\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import nvidia.cudnn\n",
    "pasta=nvidia.cudnn.__file__\n",
    "print(pasta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb3ffbbe-a94c-4403-88fa-79bee1387585",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "os.environ[\"TUNE_DISABLE_STRICT_METRIC_CHECKING\"] = \"1\"\n",
    "os.environ[\"TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S\"] = \"2\"\n",
    "#os.environ[\"LD_LIBRARY_PATH\"]=\"/usr/local/cuda-12.4/targets/x86_64-linux/include:/opt/conda/lib/python3.11/site-packages/tensorrt_bindings/:/opt/conda/lib/python3.11/site-packages/tensorrt_libs:/usr/lib/python3.10/dist-packages/tensorrt:\"+os.environ[\"LD_LIBRARY_PATH\"]\n",
    "os.environ['PYAV_LOGGING']='off'\n",
    "import sys\n",
    "sys.setrecursionlimit(10000)  # Increase cautiously\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df0c62ae-fc65-4cf1-8de5-79535beed109",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ii  graphsurgeon-tf                                    8.6.1.6-1+cuda12.0                      amd64        GraphSurgeon for TensorRT package\n",
      "ii  libnvinfer-bin                                     10.0.1.6-1+cuda12.4                     amd64        TensorRT binaries\n",
      "ii  libnvinfer-dev                                     10.0.1.6-1+cuda12.4                     amd64        TensorRT development libraries\n",
      "ii  libnvinfer-dispatch-dev                            10.0.1.6-1+cuda12.4                     amd64        TensorRT development dispatch runtime libraries\n",
      "ii  libnvinfer-dispatch10                              10.0.1.6-1+cuda12.4                     amd64        TensorRT dispatch runtime library\n",
      "ii  libnvinfer-headers-dev                             10.0.1.6-1+cuda12.4                     amd64        TensorRT development headers\n",
      "ii  libnvinfer-headers-plugin-dev                      10.0.1.6-1+cuda12.4                     amd64        TensorRT plugin headers\n",
      "ii  libnvinfer-lean-dev                                10.0.1.6-1+cuda12.4                     amd64        TensorRT lean runtime libraries\n",
      "ii  libnvinfer-lean10                                  10.0.1.6-1+cuda12.4                     amd64        TensorRT lean runtime library\n",
      "ii  libnvinfer-plugin-dev                              10.0.1.6-1+cuda12.4                     amd64        TensorRT plugin libraries\n",
      "ii  libnvinfer-plugin10                                10.0.1.6-1+cuda12.4                     amd64        TensorRT plugin libraries\n",
      "ii  libnvinfer-samples                                 10.0.1.6-1+cuda12.4                     all          TensorRT samples\n",
      "ii  libnvinfer-vc-plugin-dev                           10.0.1.6-1+cuda12.4                     amd64        TensorRT vc-plugin library\n",
      "ii  libnvinfer-vc-plugin10                             10.0.1.6-1+cuda12.4                     amd64        TensorRT vc-plugin library\n",
      "ii  libnvinfer10                                       10.0.1.6-1+cuda12.4                     amd64        TensorRT runtime libraries\n",
      "ii  libnvonnxparsers-dev                               10.0.1.6-1+cuda12.4                     amd64        TensorRT ONNX libraries\n",
      "ii  libnvonnxparsers10                                 10.0.1.6-1+cuda12.4                     amd64        TensorRT ONNX libraries\n",
      "ii  onnx-graphsurgeon                                  10.0.1.6-1+cuda12.4                     amd64        ONNX GraphSurgeon for TensorRT package\n",
      "ii  python3-libnvinfer                                 10.0.1.6-1+cuda12.4                     amd64        Python 3 bindings for TensorRT standard runtime\n",
      "ii  python3-libnvinfer-dev                             10.0.1.6-1+cuda12.4                     amd64        Python 3 development package for TensorRT standard runtime\n",
      "ii  python3-libnvinfer-dispatch                        10.0.1.6-1+cuda12.4                     amd64        Python 3 bindings for TensorRT dispatch runtime\n",
      "ii  python3-libnvinfer-lean                            10.0.1.6-1+cuda12.4                     amd64        Python 3 bindings for TensorRT lean runtime\n",
      "ii  tensorrt                                           10.0.1.6-1+cuda12.4                     amd64        Meta package for TensorRT\n",
      "ii  uff-converter-tf                                   8.6.1.6-1+cuda12.0                      amd64        UFF converter for TensorRT package\n"
     ]
    }
   ],
   "source": [
    "!dpkg -l | grep TensorRT  # For Debian/Ubuntu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9ab615a-14f0-4976-9f2a-c574c5a83b71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-20 21:47:11.607414: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-20 21:47:13.030300: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.16.1\n",
      "Number of GPUs Available: 1\n",
      "TensorRT version: 10.1.0\n",
      "TensorRT version: /opt/conda/lib/python3.11/site-packages/tensorrt/__init__.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-20 21:47:14.611808: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-20 21:47:14.634402: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-20 21:47:14.634457: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"Number of GPUs Available:\", len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "# Assuming TensorRT Python API is installed (`tensorrt` package)\n",
    "import tensorrt as trt\n",
    "print(\"TensorRT version:\", trt.__version__)\n",
    "print(\"TensorRT version:\", trt.__file__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11ef30fe-3ee0-4499-bbac-ba76929cd993",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/bin:/root/gems/bin:/opt/mssql-tools18/bin:/usr/local/cuda-12.4/bin:/opt/conda/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/conda/bin:/opt/conda/lib/python3.11/site-packages/nvidia/cudnn:/datatensor:/usr/lib/x86_64-linux-gnu:/opt/conda/lib/python3.11/site-packages/torch/lib:/opt/conda/condabin:/opt/conda/lib:/opt/conda/bin:/opt/conda/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/root/mambaforge/lib:/opt/conda/lib/python3.11/site-packages/tensorflow/include/third_party/gpus/cuda/include/:/opt/conda/lib/python3.11/site-packages/nvidia/cuda_runtime/lib:/opt/conda/lib:/usr/lib/x86_64-linux-gnu/:/usr/local/cuda-12.2/compat/:/opt/conda/lib/:/opt/conda/lib:/opt/conda/lib/python3.11/site-packages/nvidia/cuda_runtime/lib:/root/.julia/juliaup/julia-1.10.4+0.x64.linux.gnu/bin/:/opt/conda/lib/python3.11/site-packages/nvidia/cuda_runtime/lib/:/opt/mssql-tools18/bin:/usr/local/cuda-12.4/bin:/usr/local/cuda-12.4/targets/x86_64-linux/include:/usr/local/cuda/include/\n"
     ]
    }
   ],
   "source": [
    "!echo $PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63d205c9-084d-4359-8680-002b667db59c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/cuda-12.4/lib64:/usr/local/cuda-12.4/targets/x86_64-linux/include:/usr/local/cuda/include/:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/opt/conda/lib/python3.11/site-packages/tensorrt_bindings/:/opt/conda/lib/python3.11/site-packages/tensorrt_libs:/usr/lib/python3.10/dist-packages/tensorrt:/usr/local/cuda-12.4/lib64:/usr/local/cuda-12.4/targets/x86_64-linux/lib/:/opt/conda/lib:/opt/conda/lib/python3.11/site-packages/nvidia/cudnn/lib:/usr/lib/x86_64-linux-gnu:/usr/local/cuda-12.4/include:/usr/local/cuda-12.4:/usr/local/cuda-12.4/targets/x86_64-linux/lib/:/lib:/lib/:/usr/local/cuda/extras/CUPTI/lib64:/usr/lib/x86_64-linux-gnu/:/usr/lib/x86_64-linux-gnu/odbc/:/usr/lib/x86_64-linux-gnu/:/opt/conda/lib/python3.11/site-packages/nvidia/cuda_runtime/lib:/opt/conda/lib:/usr/lib/x86_64-linux-gnu/:/usr/local/cuda-12.2/compat/:/opt/conda/lib/:/opt/conda/lib:/opt/conda/lib/python3.11/site-packages/nvidia/cuda_runtime/lib:/opt/conda/lib/python3.11/site-packages/nvidia/cublas/include:/opt/conda/lib/python3.11/site-packages/nvidia/cusparse/lib:/opt/conda/lib/python3.11/site-packages/nvidia/cufft/lib:/opt/conda/lib/python3.11/site-packages/nvidia/cufft/lib:/opt/conda/lib/python3.11/site-packages/tensorflow/include/third_party/gpus/cuda/include:/root/.julia/juliaup/julia-1.10.4+0.x64.linux.gnu/lib/julia/:/usr/lib/x86_64-linux-gnu:/opt/conda/lib:/opt/conda/lib/python3.11/site-packages/torch/lib:/opt/conda/lib/python3.11/site-packages/nvidia/cuda_runtime/lib/:/opt/conda/lib/python3.11/site-packages/tensorrt:/opt/conda/lib/:/opt/conda/lib/python3.11/site-packages/nvidia/cudnn/lib:/opt/conda/lib/python3.11/site-packages/cupy:/opt/conda/lib/python3.11/site-packages/cupy/cuda:/root/mambaforge/lib:/opt/conda/lib/python3.11/site-packages/torch/lib/:/opt/conda/lib/python3.11/site-packages/torch/lib/:/opt/conda/lib:/opt/conda/lib/python3.11/site-packages/nvidia/cuda_runtime/lib/\n",
      "/root/gems\n",
      "/root\n"
     ]
    }
   ],
   "source": [
    "!echo $LD_LIBRARY_PATH\n",
    "!echo $GEM_HOME\n",
    "!echo $HOME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0851b0b-958c-4ddb-8b8d-b2d82ec16738",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.1.0\n"
     ]
    }
   ],
   "source": [
    "import tensorrt\n",
    "print(tensorrt.__version__)\n",
    "#assert tensorrt.Builder(tensorrt.Logger())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f34fded8-1b89-4685-84c6-2ebaa7c47745",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importações necessárias\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments\n",
    "import datasets\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.train import RunConfig\n",
    "from ray.tune import CLIReporter\n",
    "from ray.tune.schedulers import ASHAScheduler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b01db1a-42c7-499f-8fac-aa982abdc7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filepath, target_position=-1, date_column=None, encoding=None):\n",
    "    \"\"\"\n",
    "    Carrega dados de um arquivo, tentando várias codificações em caso de falha.\n",
    "    \n",
    "    Args:\n",
    "    - filepath: Caminho para o arquivo de dados.\n",
    "    - target_position: Posição da coluna target, padrão é -1 (última coluna).\n",
    "    - date_column: Nome da coluna de data para ordenar os dados, se aplicável.\n",
    "    - encoding: Codificação do arquivo de dados. Padrão é None, tentará 'utf-8', 'ISO-8859-1' e 'windows-1252'.\n",
    "    \n",
    "    Returns:\n",
    "    - X_train, X_test, y_train, y_test: Dados divididos em conjuntos de treino e teste.\n",
    "    \"\"\"\n",
    "    # Tentativas de codificações\n",
    "    encodings = ['utf-8', 'ISO-8859-1', 'windows-1252'] if encoding is None else [encoding]\n",
    "\n",
    "    for enc in encodings:\n",
    "        try:\n",
    "            if filepath.lower().endswith('.csv'):\n",
    "                df = pd.read_csv(filepath, encoding=enc)\n",
    "            elif filepath.lower().endswith('.xlsx') or filepath.lower().endswith('.xls'):\n",
    "                try:\n",
    "                    df = pd.read_excel(filepath, engine='openpyxl')\n",
    "                except ValueError:\n",
    "                    df = pd.read_excel(filepath, engine='xlrd')\n",
    "            else:\n",
    "                raise ValueError(\"Formato de arquivo não suportado!\")\n",
    "\n",
    "            # Verificar se a coluna de data existe e ordenar os dados por ela\n",
    "            if date_column and date_column in df.columns:\n",
    "                df[date_column] = pd.to_datetime(df[date_column])\n",
    "                df.sort_values(date_column, inplace=True)\n",
    "\n",
    "            # Separação das features e target\n",
    "            X = df.iloc[:, :target_position]  # Todas as colunas exceto a última\n",
    "            y = df.iloc[:, target_position]   # A última coluna é considerada como target\n",
    "\n",
    "            # Divisão dos dados em treino e teste\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "            return X_train, X_test, y_train, y_test\n",
    "\n",
    "        except UnicodeDecodeError:\n",
    "            print(f\"Falha ao decodificar com {enc}, tentando outra codificação...\")\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao carregar o arquivo com codificação {enc}: {e}\")\n",
    "            continue  # Tenta a próxima codificação\n",
    "\n",
    "    print(\"Não foi possível carregar o arquivo com as codificações tentadas.\")\n",
    "    return None, None, None, None\n",
    "\n",
    "# Exemplo de uso da função\n",
    "#X_train, X_test, y_train, y_test = load_data('your_data_file.xlsx', date_column='optional_date_column')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f041f8b8-ba71-49f0-9cf0-07b7304eb9e4",
   "metadata": {},
   "source": [
    "# Função para configurar e treinar modelos ORIGINAL\n",
    "# Função para configurar e treinar modelos\n",
    "def train_and_evaluate(model_name, config):\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "        # Tokenização\n",
    "        train_encodings = tokenizer(X_train.tolist(), truncation=True, padding=True, max_length=config.get(\"max_length\", 512))\n",
    "        test_encodings = tokenizer(X_test.tolist(), truncation=True, padding=True, max_length=config.get(\"max_length\", 512))\n",
    "\n",
    "        # Criação de datasets\n",
    "        train_dataset = datasets.Dataset.from_dict({'input_ids': train_encodings['input_ids'], 'labels': y_train.tolist()})\n",
    "        test_dataset = datasets.Dataset.from_dict({'input_ids': test_encodings['input_ids'], 'labels': y_test.tolist()})\n",
    "\n",
    "        # Configurações de treinamento\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=f'./results/{model_name}',\n",
    "            num_train_epochs=config[\"num_epochs\"],\n",
    "            learning_rate=config['lr'],\n",
    "            per_device_train_batch_size=config['batch_size'],\n",
    "            evaluation_strategy='epoch'\n",
    "        )\n",
    "\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=test_dataset,\n",
    "            compute_metrics=lambda p: {'accuracy': accuracy_score(p.label_ids, np.argmax(p.predictions, axis=1))}\n",
    "        )\n",
    "\n",
    "        # Treinamento e avaliação\n",
    "        trainer.train()\n",
    "        eval_results = trainer.evaluate()\n",
    "\n",
    "        # Reportar a perda de avaliação para o Ray Tune\n",
    "        tune.CLIReporter(metric_columns=[loss=eval_results['eval_loss'], accuracy=eval_results['eval_metrics']['accuracy']])\n",
    "        #reporter = CLIReporter(metric_columns=[\"accuracy\", \"loss\"])  # Adjust columns based on your metrics\n",
    "\n",
    "        # Matriz de confusão\n",
    "        preds_output = trainer.predict(test_dataset)\n",
    "        preds = np.argmax(preds_output.predictions, axis=-1)\n",
    "        cm = confusion_matrix(y_test, preds)\n",
    "        plt.figure(figsize=(7,5))\n",
    "        sns.heatmap(cm, annot=True, fmt='g', cmap='Blues', xticklabels=['Non-Fraud', 'Fraud'], yticklabels=['Non-Fraud', 'Fraud'])\n",
    "        plt.xlabel('Predicted labels')\n",
    "        plt.ylabel('True labels')\n",
    "        plt.title(f'Confusion Matrix for {model_name}')\n",
    "        plt.show()\n",
    "\n",
    "        print(f\"Model: {model_name}, Accuracy: {accuracy_score(y_test, preds)}\")\n",
    "        return eval_results['eval_loss']\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error training model {model_name}: {e}\")\n",
    "        # Optionally, return a default or error indicator result\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3c35c3d7-4ce7-4e83-bdad-5c757184f4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "import datasets\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from ray import tune\n",
    "\n",
    "# Assume X_train, X_test, y_train, y_test are pre-defined elsewhere in your script\n",
    "\n",
    "# Function to configure and train models original\n",
    "def train_and_evaluate(model_name, config):\n",
    "    try:\n",
    "        # Load tokenizer and model from Hugging Face model hub\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "        # Tokenization\n",
    "        train_encodings = tokenizer(X_train, truncation=True, padding=True, max_length=config.get(\"max_length\", 512))\n",
    "        test_encodings = tokenizer(X_test, truncation=True, padding=True, max_length=config.get(\"max_length\", 512))\n",
    "\n",
    "        # Creation of datasets\n",
    "        train_dataset = datasets.Dataset.from_dict({'input_ids': train_encodings['input_ids'], 'labels': y_train})\n",
    "        test_dataset = datasets.Dataset.from_dict({'input_ids': test_encodings['input_ids'], 'labels': y_test})\n",
    "\n",
    "        # Training configurations\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=f'./results/{model_name}',  # Output directory\n",
    "            num_train_epochs=config[\"num_epochs\"],  # Number of training epochs\n",
    "            learning_rate=config['lr'],  # Learning rate\n",
    "            per_device_train_batch_size=config['batch_size'],  # Batch size per device\n",
    "            evaluation_strategy='epoch'  # Evaluate at the end of each epoch\n",
    "        )\n",
    "\n",
    "        # Trainer setup\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=test_dataset,\n",
    "            compute_metrics=lambda p: {'accuracy': accuracy_score(p.label_ids, np.argmax(p.predictions, axis=1))}\n",
    "        )\n",
    "\n",
    "        # Training and evaluation\n",
    "        trainer.train()\n",
    "        eval_results = trainer.evaluate()\n",
    "\n",
    "        # Reporting the evaluation loss and accuracy to Ray Tune\n",
    "        tune.CLIReporter(metric_columns=[loss:=eval_results['eval_loss'], accuracy:=eval_results['eval_metrics']['accuracy']])\n",
    "        \n",
    "\n",
    "        # Confusion matrix visualization\n",
    "        preds_output = trainer.predict(test_dataset)\n",
    "        preds = np.argmax(preds_output.predictions, axis=-1)\n",
    "        cm = confusion_matrix(y_test, preds)\n",
    "        plt.figure(figsize=(7,5))\n",
    "        sns.heatmap(cm, annot=True, fmt='g', cmap='Blues', xticklabels=['Non-Fraud', 'Fraud'], yticklabels=['Non-Fraud', 'Fraud'])\n",
    "        plt.xlabel('Predicted labels')\n",
    "        plt.ylabel('True labels')\n",
    "        plt.title(f'Confusion Matrix for {model_name}')\n",
    "        plt.show()\n",
    "\n",
    "        print(f\"Model: {model_name}, Accuracy: {accuracy_score(y_test, preds)}\")\n",
    "        return eval_results['eval_loss']\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error training model {model_name}: {e}\")\n",
    "        # Report worst-case performance metrics to avoid trial hanging or crashing\n",
    "        tune.CLIReporter(metric_columns=[loss:=float('inf'), accuracy:=0])\n",
    "        #tune.CLIReporter(metric_columns=[loss=eval_results['eval_loss'], accuracy=eval_results['eval_metrics']['accuracy']])\n",
    "        return float('inf')  # Return the worst possible loss in case of failure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a0cc678c-f12a-4fdf-ad48-8ca14ebf2111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuração e execução da otimização de hiperparâmetros com Ray Tune\n",
    "def optimize_and_train(model_name):\n",
    "    def train_model(config):\n",
    "        return train_and_evaluate(model_name, config)\n",
    "\n",
    "    config = {\n",
    "        \"lr\": tune.loguniform(1e-5, 5e-5),\n",
    "        \"batch_size\": tune.choice([8, 16, 32]),\n",
    "        \"num_epochs\": tune.choice([2, 3, 4]),\n",
    "        \"max_length\": tune.choice([128, 256, 512])\n",
    "    }\n",
    "\n",
    "    scheduler = ASHAScheduler(\n",
    "        metric=\"loss\",  # or another metric like \"accuracy\"\n",
    "        mode=\"min\",     # use \"max\" if you need to maximize the metric\n",
    "        max_t=10,\n",
    "        grace_period=1,\n",
    "        reduction_factor=2\n",
    "    )\n",
    "    result = tune.run(\n",
    "        train_model,\n",
    "        resources_per_trial={\"cpu\": 8, \"gpu\": 1},\n",
    "        config=config,\n",
    "        num_samples=10,\n",
    "        scheduler=scheduler\n",
    "    )\n",
    "\n",
    "    # Acessar o melhor trial e seu último resultado\n",
    "    print(result.get_best_result(metric=\"loss\", mode=\"min\").config)\n",
    "\n",
    "    \n",
    "#  best_trial = result.get_best_trial(\"loss\", \"min\", \"last\")\n",
    "#    if 'loss' in best_trial.last:\n",
    "#        print(\"Best trial final validation loss: \", best_trial.last['loss'])\n",
    "#        print(\"Best checkpoint path: \", best_trial.last[\"checkpoint\"])\n",
    "#    else:\n",
    "#        print(\"Loss metric not found in the best trial results.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cc50a358-bd37-436f-a808-4e9ac6f826a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar dados\n",
    "X_train, X_test, y_train, y_test = load_data('Galerias.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5d9a2264-df9d-4d47-a202-e159df639f78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage stats collection is enabled by default for nightly wheels. To disable this, run the following command: `ray disable-usage-stats` before starting Ray. See https://docs.ray.io/en/master/cluster/usage-stats.html for more details.\n"
     ]
    },
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xf8 in position 0: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Iniciar Ray\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mignore_reinit_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/ray/_private/client_mode_hook.py:103\u001b[0m, in \u001b[0;36mclient_mode_hook.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minit\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m is_client_mode_enabled_by_default:\n\u001b[1;32m    102\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(ray, func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/ray/_private/worker.py:1663\u001b[0m, in \u001b[0;36minit\u001b[0;34m(address, num_cpus, num_gpus, resources, labels, object_store_memory, local_mode, ignore_reinit_error, include_dashboard, dashboard_host, dashboard_port, job_config, configure_logging, logging_level, logging_format, logging_config, log_to_driver, namespace, runtime_env, storage, **kwargs)\u001b[0m\n\u001b[1;32m   1630\u001b[0m     ray_params \u001b[38;5;241m=\u001b[39m ray\u001b[38;5;241m.\u001b[39m_private\u001b[38;5;241m.\u001b[39mparameter\u001b[38;5;241m.\u001b[39mRayParams(\n\u001b[1;32m   1631\u001b[0m         node_ip_address\u001b[38;5;241m=\u001b[39m_node_ip_address,\n\u001b[1;32m   1632\u001b[0m         object_ref_seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1657\u001b[0m         node_name\u001b[38;5;241m=\u001b[39m_node_name,\n\u001b[1;32m   1658\u001b[0m     )\n\u001b[1;32m   1659\u001b[0m     \u001b[38;5;66;03m# Start the Ray processes. We set shutdown_at_exit=False because we\u001b[39;00m\n\u001b[1;32m   1660\u001b[0m     \u001b[38;5;66;03m# shutdown the node in the ray.shutdown call that happens in the atexit\u001b[39;00m\n\u001b[1;32m   1661\u001b[0m     \u001b[38;5;66;03m# handler. We still spawn a reaper process in case the atexit handler\u001b[39;00m\n\u001b[1;32m   1662\u001b[0m     \u001b[38;5;66;03m# isn't called.\u001b[39;00m\n\u001b[0;32m-> 1663\u001b[0m     _global_node \u001b[38;5;241m=\u001b[39m \u001b[43mray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_private\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mNode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1664\u001b[0m \u001b[43m        \u001b[49m\u001b[43mray_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mray_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1665\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1666\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshutdown_at_exit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1667\u001b[0m \u001b[43m        \u001b[49m\u001b[43mspawn_reaper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mray_init_cluster\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1669\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1670\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1671\u001b[0m     \u001b[38;5;66;03m# In this case, we are connecting to an existing cluster.\u001b[39;00m\n\u001b[1;32m   1672\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m num_cpus \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m num_gpus \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/ray/_private/node.py:336\u001b[0m, in \u001b[0;36mNode.__init__\u001b[0;34m(self, ray_params, head, shutdown_at_exit, spawn_reaper, connect_only, default_worker, ray_init_cluster)\u001b[0m\n\u001b[1;32m    333\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart_head_processes()\n\u001b[1;32m    335\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m connect_only:\n\u001b[0;32m--> 336\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart_ray_processes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    337\u001b[0m     \u001b[38;5;66;03m# we should update the address info after the node has been started\u001b[39;00m\n\u001b[1;32m    338\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/ray/_private/node.py:1396\u001b[0m, in \u001b[0;36mNode.start_ray_processes\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1392\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_config \u001b[38;5;241m=\u001b[39m new_config\n\u001b[1;32m   1394\u001b[0m \u001b[38;5;66;03m# Make sure we don't call `determine_plasma_store_config` multiple\u001b[39;00m\n\u001b[1;32m   1395\u001b[0m \u001b[38;5;66;03m# times to avoid printing multiple warnings.\u001b[39;00m\n\u001b[0;32m-> 1396\u001b[0m resource_spec \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_resource_spec\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1397\u001b[0m (\n\u001b[1;32m   1398\u001b[0m     plasma_directory,\n\u001b[1;32m   1399\u001b[0m     object_store_memory,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1403\u001b[0m     huge_pages\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ray_params\u001b[38;5;241m.\u001b[39mhuge_pages,\n\u001b[1;32m   1404\u001b[0m )\n\u001b[1;32m   1405\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart_raylet(plasma_directory, object_store_memory)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/ray/_private/node.py:580\u001b[0m, in \u001b[0;36mNode.get_resource_spec\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    563\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAutoscaler overriding resources: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00menv_resources\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    564\u001b[0m     (\n\u001b[1;32m    565\u001b[0m         num_cpus,\n\u001b[1;32m    566\u001b[0m         num_gpus,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    569\u001b[0m         resources,\n\u001b[1;32m    570\u001b[0m     ) \u001b[38;5;241m=\u001b[39m merge_resources(env_resources, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ray_params\u001b[38;5;241m.\u001b[39mresources)\n\u001b[1;32m    571\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_resource_spec \u001b[38;5;241m=\u001b[39m \u001b[43mResourceSpec\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    572\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ray_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_cpus\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mnum_cpus\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mnum_cpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    573\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ray_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_gpus\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mnum_gpus\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mnum_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    574\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ray_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmemory\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmemory\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmemory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    575\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ray_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobject_store_memory\u001b[49m\n\u001b[1;32m    576\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobject_store_memory\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[1;32m    577\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobject_store_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    578\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresources\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    579\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ray_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mredis_max_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m--> 580\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresolve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mis_head\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhead\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode_ip_address\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnode_ip_address\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    581\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_resource_spec\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/ray/_private/resource_spec.py:215\u001b[0m, in \u001b[0;36mResourceSpec.resolve\u001b[0;34m(self, is_head, node_ip_address)\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    212\u001b[0m     resources[accelerator_resource_name] \u001b[38;5;241m=\u001b[39m num_accelerators\n\u001b[1;32m    214\u001b[0m accelerator_type \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 215\u001b[0m     \u001b[43maccelerator_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_current_node_accelerator_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m )\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m accelerator_type:\n\u001b[1;32m    218\u001b[0m     resources[\n\u001b[1;32m    219\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mray_constants\u001b[38;5;241m.\u001b[39mRESOURCE_CONSTRAINT_PREFIX\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00maccelerator_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    220\u001b[0m     ] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/ray/_private/accelerators/nvidia_gpu.py:71\u001b[0m, in \u001b[0;36mNvidiaGPUAcceleratorManager.get_current_node_accelerator_type\u001b[0;34m()\u001b[0m\n\u001b[1;32m     69\u001b[0m     device_name \u001b[38;5;241m=\u001b[39m pynvml\u001b[38;5;241m.\u001b[39mnvmlDeviceGetName(handle)\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(device_name, \u001b[38;5;28mbytes\u001b[39m):\n\u001b[0;32m---> 71\u001b[0m         device_name \u001b[38;5;241m=\u001b[39m \u001b[43mdevice_name\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     72\u001b[0m     cuda_device_type \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     73\u001b[0m         NvidiaGPUAcceleratorManager\u001b[38;5;241m.\u001b[39m_gpu_name_to_accelerator_type(device_name)\n\u001b[1;32m     74\u001b[0m     )\n\u001b[1;32m     75\u001b[0m pynvml\u001b[38;5;241m.\u001b[39mnvmlShutdown()\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xf8 in position 0: invalid start byte"
     ]
    }
   ],
   "source": [
    "# Iniciar Ray\n",
    "ray.init(ignore_reinit_error=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98782e7a-a5a8-4d64-8988-e283d8affae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista de modelos a serem avaliados\n",
    "models = ['bert-base-uncased', 'roberta-base', 'google/electra-small-discriminator']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aed971c-4490-407b-92f7-e6dca1518c80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c64168-dfb1-4085-a72d-261b9b9f17ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop para treinar e otimizar cada modelo\n",
    "for model in models:\n",
    "    print(f\"Training and optimizing {model}\")\n",
    "    optimize_and_train(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52307dd3-8d3a-4bb0-a70a-2920bd83ddbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finalizar Ray\n",
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4d69f8-674d-4bd4-b43b-5a94bed5f562",
   "metadata": {},
   "outputs": [],
   "source": [
    "!dpkg -l | grep TensorRT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67c9bbd-2217-4b7a-b4c4-013b286d188b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Verifica a disponibilidade de GPUs\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Configura o TensorFlow para usar apenas a primeira GPU\n",
    "        tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "        print(\"GPUs available:\", gpus)\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "else:\n",
    "    print(\"No GPU available.\")\n",
    "\n",
    "# Testa uma operação simples de TensorFlow na GPU\n",
    "with tf.device('/GPU:0'):  # Força a execução na GPU\n",
    "    random_matrix = tf.random.uniform([1000, 1000])\n",
    "    result = tf.matmul(random_matrix, tf.transpose(random_matrix))\n",
    "    print(\"Success: TensorFlow ran a matrix multiplication on GPU.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885ca7b9-0032-4b4e-8992-290961db6eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pycuda.driver as cuda\n",
    "import pycuda.autoinit  # Inicializa CUDA automaticamente\n",
    "from pycuda.compiler import SourceModule\n",
    "\n",
    "def test_pycuda():\n",
    "    mod = SourceModule(\"\"\"\n",
    "    __global__ void multiply_them(float *dest, float *a, float *b)\n",
    "    {\n",
    "        const int i = threadIdx.x;\n",
    "        dest[i] = a[i] * b[i];\n",
    "    }\n",
    "    \"\"\")\n",
    "\n",
    "    # Obtendo a função do módulo compilado\n",
    "    multiply_them = mod.get_function(\"multiply_them\")\n",
    "\n",
    "    import numpy as np\n",
    "    a = np.random.randn(400).astype(np.float32)\n",
    "    b = np.random.randn(400).astype(np.float32)\n",
    "\n",
    "    dest = np.zeros_like(a)\n",
    "\n",
    "    # Chamando a função do kernel\n",
    "    multiply_them(cuda.Out(dest), cuda.In(a), cuda.In(b), block=(400,1,1), grid=(1,1))\n",
    "\n",
    "    print(\"Test completed: \", np.allclose(dest, a * b))\n",
    "\n",
    "test_pycuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7183c6dd-7b79-4189-adeb-2caf2053ce15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Verifica se o TensorRT está disponível no TensorFlow\n",
    "try:\n",
    "    from tensorflow.python.compiler.tensorrt import trt  # Verifica se o TensorRT está disponível\n",
    "    print(\"TensorRT está disponível no TensorFlow.\")\n",
    "except ImportError:\n",
    "    print(\"TensorRT não está disponível no TensorFlow. Verifique sua instalação.\")\n",
    "\n",
    "# Verifica se o TensorRT GPU está disponível\n",
    "if tf.test.is_gpu_available():\n",
    "    print(\"TensorRT GPU está disponível.\")\n",
    "else:\n",
    "    print(\"TensorRT GPU não está disponível.\")\n",
    "\n",
    "# Exibe a versão do TensorFlow GPU\n",
    "print(\"Versão do TensorFlow GPU:\", tf.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc062e74-c90d-4736-9fd3-0c0d0dc5d06d",
   "metadata": {},
   "source": [
    "!pip install transformers onnx onnx-tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38fce10-23c7-4bcd-8b71-11ee45cfc749",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
